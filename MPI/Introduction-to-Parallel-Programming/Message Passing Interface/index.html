

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="刘笑笑">
  <meta name="keywords" content="java、go、操作系统、网络">
  
    <meta name="description" content="第一节、Welcome to Week 3一、基本介绍本周介绍的消息传递接口（MPI）将介绍通信器和进程如何通信消息。发送和接收消息可以用于一对一的通信。一对多的广播和分散的消息可以通过减少和收集操作来分发和收集回来。所有这些经常使用的集体操作，都由MPI库提供。我们将通过许多例子和执行来介绍这些MPI功能的差异，你可以进一步探索。 二、Communicator in MPI首先，我们需要了解MP">
<meta property="og:type" content="article">
<meta property="og:title" content="Message Passing Interface">
<meta property="og:url" content="http://by.ecel.top/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/index.html">
<meta property="og:site_name" content="Hello">
<meta property="og:description" content="第一节、Welcome to Week 3一、基本介绍本周介绍的消息传递接口（MPI）将介绍通信器和进程如何通信消息。发送和接收消息可以用于一对一的通信。一对多的广播和分散的消息可以通过减少和收集操作来分发和收集回来。所有这些经常使用的集体操作，都由MPI库提供。我们将通过许多例子和执行来介绍这些MPI功能的差异，你可以进一步探索。 二、Communicator in MPI首先，我们需要了解MP">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://by.ecel.top/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204105843994.png">
<meta property="og:image" content="http://by.ecel.top/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204110444478.png">
<meta property="og:image" content="http://by.ecel.top/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204111726590.png">
<meta property="og:image" content="http://by.ecel.top/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204164807103.png">
<meta property="og:image" content="http://by.ecel.top/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204164824037.png">
<meta property="og:image" content="http://by.ecel.top/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204165627157.png">
<meta property="og:image" content="http://by.ecel.top/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204203725379.png">
<meta property="og:image" content="http://by.ecel.top/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204215929294.png">
<meta property="og:image" content="http://by.ecel.top/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221208170337715.png">
<meta property="og:image" content="http://by.ecel.top/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221208170844247.png">
<meta property="article:published_time" content="2022-12-04T01:32:47.000Z">
<meta property="article:modified_time" content="2022-12-10T02:40:35.974Z">
<meta property="article:author" content="刘笑笑">
<meta property="article:tag" content="并行计算">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://by.ecel.top/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204105843994.png">
  
  
  
  <title>Message Passing Interface - Hello</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"by.ecel.top","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0,"license":"BY","author":{"enable":true},"post_date":{"enable":true,"format":"LL"},"update_date":{"enable":false,"format":"LL"}},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  

  

  



  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Liuxiaoxiao</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Message Passing Interface"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-12-04 09:32" pubdate>
          2022年12月4日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          22k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          184 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Message Passing Interface</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：2022年12月10日 上午
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <h2 id="第一节、Welcome-to-Week-3"><a href="#第一节、Welcome-to-Week-3" class="headerlink" title="第一节、Welcome to Week 3"></a>第一节、Welcome to Week 3</h2><h3 id="一、基本介绍"><a href="#一、基本介绍" class="headerlink" title="一、基本介绍"></a>一、基本介绍</h3><p>本周介绍的消息传递接口（MPI）将介绍通信器和进程如何通信消息。发送和接收消息可以用于一对一的通信。一对多的广播和分散的消息可以通过减少和收集操作来分发和收集回来。所有这些经常使用的集体操作，都由MPI库提供。我们将通过许多例子和执行来介绍这些MPI功能的差异，你可以进一步探索。</p>
<h3 id="二、Communicator-in-MPI"><a href="#二、Communicator-in-MPI" class="headerlink" title="二、Communicator in MPI"></a>二、Communicator in MPI</h3><p>首先，我们需要了解MPI中的通信器是什么。当我们启动MPI时，整个环境将所有的进程和参与这个应用程序的内核放在一起，并将它们捆绑在一起，这就是所谓的通信器。通信器就像一个集合，将所有的进程绑定在一起，并证实只有在一个应用程序中在一起的进程才能相互通信。我们最常使用的默认通信器是</p>
<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">MPI_COMM_WORLD</span><br></code></pre></td></tr></table></figure>

<p>已经在头文件中预定义了。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">mpi.h<br></code></pre></td></tr></table></figure>

<img src="/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204105843994.png" srcset="/img/loading.gif" lazyload alt="image-20221204105843994" style="zoom:50%;">

<h4 id="Ranks-and-Size"><a href="#Ranks-and-Size" class="headerlink" title="Ranks and Size"></a>Ranks and Size</h4><p>如前所述，当我们将需要使用一个通信器时，我们经常使用MPI_COMM_WORLD。一旦我们实际初始化MPI环境，所有的进程就会在通信器中。正如我们可以预测的那样，能够区分不同的进程是很好的，这就是行列的作用。<strong>当我们初始化环境时，MPI通信器将为每个进程分配一个数字。这就是所谓的等级。</strong>它是一个数字，从零开始，以大小减一结束。在这个例子中，正如你所看到的，我们已经启动了有七个内核的应用程序，每个内核都有一个等级。因此，在这个应用程序中，我们有不同的进程，它们被赋予从零到六的等级。</p>
<p>这有助于我们识别和使用一个特定的处理器。例如，如果我们想让2号处理器执行某项任务，我们可以用条件来命令执行任务：如果等级是2。 在下面的小节中，当我们进一步学习在特定处理器之间发送消息时，我们将看到等级是如何有用的。比如说，如果等级等于2，就向等级6发送一个消息，这很有用。这就是我们在MPI中不同进程之间发送和接收消息的方式。</p>
<p>为了实际得到这些数字，我们在MPI中有两个基本的例程。我们现在知道，当我们初始化MPI环境时，rank是每个处理器被赋予的数字，是你可以识别一个进程的数字。size告诉我们一个通信器中包含的进程数，或者简单地说，我们的应用程序中有多少个进程。例如，当规模为10时，等级从0到9，以此类推。</p>
<p>There are two basic routines for the <code>rank</code></p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-constructor">MPI_Comm_rank(MPI_Comm <span class="hljs-params">comm</span>, <span class="hljs-params">int</span> <span class="hljs-operator">*</span><span class="hljs-params">rank</span>)</span><br></code></pre></td></tr></table></figure>

<p>and for the <code>size</code></p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-constructor">MPI_Comm_size(MPI_Comm <span class="hljs-params">comm</span>, <span class="hljs-params">int</span> <span class="hljs-operator">*</span><span class="hljs-params">size</span>)</span>;<br></code></pre></td></tr></table></figure>

<img src="/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204110444478.png" srcset="/img/loading.gif" lazyload alt="image-20221204110444478" style="zoom:50%;">



<h3 id="三、Hello-World-2-0"><a href="#三、Hello-World-2-0" class="headerlink" title="三、Hello World 2.0"></a>三、Hello World 2.0</h3><p>在这个练习中，你将用提供的C、Python和Fortran的骨架创建你的第一个MPI程序。</p>
<p>进入练习，修改 “Hello world “的骨架，以使</p>
<ol>
<li>每个进程写出它的等级和MPI_COMM_WORLD的大小</li>
<li>只有MPI_COMM_WORLD中排名为0的进程才会打印 “hello world”。</li>
</ol>
<p>现在用4个进程运行该程序。<br>所有MPI进程的输出有可能按照等级的顺序吗？还是没有机会保证这一点？<br>当你多次运行该程序时，你观察到了什么？在评论中告诉我们你的答案和想法。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs c">%%file hello-world.c<br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;mpi.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span><br>&#123;<br>    <span class="hljs-type">int</span> rank, size;<br>    MPI_Init(<span class="hljs-literal">NULL</span>, <span class="hljs-literal">NULL</span>);<br>    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);<br>    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);<br>    <span class="hljs-keyword">if</span> (rank == <span class="hljs-number">0</span>) &#123;<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Hello world!\n&quot;</span>);<br>    &#125;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;I am process %i out of %i.\n&quot;</span>, rank, size);<br>    MPI_Finalize();<br>&#125;<br></code></pre></td></tr></table></figure>

<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>mpicc hello-world.c -o hello-world &amp;&amp; mpirun -np <span class="hljs-number">4</span> --allow-run-<span class="hljs-keyword">as</span>-root hello-world<br></code></pre></td></tr></table></figure>

<p>output：（顺序不一致）</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">I</span> am process <span class="hljs-number">1</span> out of <span class="hljs-number">4</span>.<br><span class="hljs-attribute">I</span> am process <span class="hljs-number">2</span> out of <span class="hljs-number">4</span>.<br><span class="hljs-attribute">I</span> am process <span class="hljs-number">3</span> out of <span class="hljs-number">4</span>.<br><span class="hljs-attribute">Hello</span> world!<br><span class="hljs-attribute">I</span> am process <span class="hljs-number">0</span> out of <span class="hljs-number">4</span>.<br></code></pre></td></tr></table></figure>



<h3 id="四、Messages-and-communications"><a href="#四、Messages-and-communications" class="headerlink" title="四、Messages and communications"></a>四、Messages and communications</h3><p>到目前为止，我们已经介绍了MPI，我们使用了一些简单的例程，如等级和大小来区分不同的进程，并实际分配给它们一些数字，我们可以识别并在以后使用。但是到目前为止，我们还没有用这些知识做任何有用的事情，也就是说，我们没有在进程之间发送任何信息。这就是我们需要对MPI中的消息进行了解的地方。</p>
<p>当我们在开发不同的高级应用程序时，在某些时候我们需要从一个进程到另一个进程交换信息。通常情况下，这些信息可能是一些整数，一些其他的值，甚至是数组等等。这就是使用消息的地方。信息是在子程序之间移动的数据包。因此，如前所述，如果我们把要在进程之间共享的信息打包成一些消息，我们可以通过通信网络发送它们，这样其他进程就可以作为消息接收它们。这就是数据和信息在进程之间共享的方式。当然还有一些重要的信息，为了使消息能够有效地发送和接收，我们总是需要指定。</p>
<img src="/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204111726590.png" srcset="/img/loading.gif" lazyload alt="image-20221204111726590" style="zoom:50%;">

<p>正如我们在这个例子中所看到的，我们正试图从一个等级为0的进程向等级为2的进程发送一个消息。为了使其发挥作用，我们必须指定一些信息。</p>
<ul>
<li><strong>数据大小和类型</strong></li>
</ul>
<p>发送者需要指定正在发送的是哪种类型的数据。因此，例如，如果我们要发送一个数组，让我们说100个数字，我们需要指定其大小等于100。正如你可能已经猜到的，我们还需要提到数据的类型是什么。那么，它是否是一个字符？是一个双整数吗？等等。</p>
<ul>
<li><strong>发送或接收数据的指针</strong></li>
</ul>
<p>对于这种数据交换，我们将需要两个指针。这些指针来自于发送方，它将需要指向它自己的内存，以提到，好的，我想发送的数据在这里。然后，接收方将需要指定它想接收这些数据的内存。<br>发送进程和接收进程，即行列<br>MPI环境将需要知道谁是发送方，谁是接收方。这就是等级的作用。因此，对于我们前面的例子，我们将指定等级0是发送方，等级2是接收方。</p>
<ul>
<li><strong>信息的标签</strong></li>
</ul>
<p>我们需要指定的下一个信息是信息的标签。标签是一个简单的数字，我们可以给它分配任何数值，让接收者可以从中识别信息。例如，如果我们要发送两条信息，我们可以指定一个标签为，比方说0，另一个为标签1。这有助于接收者识别和区分信息。但通常情况下，如果我们只有一条信息，我们可以直接把标签定为0。</p>
<ul>
<li><strong>通信器，即MPI_COMM_WORLD</strong></li>
</ul>
<p>我们需要指定的最后一个参数是我们要发送消息的通信器是什么。在我们这里，它将是MPI_COMM_WORLD，但随着我们做更多的练习和实际操作，我们最终会更好地了解这些函数。</p>
<h4 id="MPI数据类型"><a href="#MPI数据类型" class="headerlink" title="MPI数据类型"></a>MPI数据类型</h4><p>MPI环境定义了它自己的基本数据类型。然而，如果你熟悉C语言，它们真的很简单，因为你要做的只是在变量前用大写字母写上MPI，然后把所有东西都改成大写。</p>
<table>
<thead>
<tr>
<th align="left">C Datatype</th>
<th align="left">MPI Datatype</th>
</tr>
</thead>
<tbody><tr>
<td align="left">char</td>
<td align="left">MPI_CHAR</td>
</tr>
<tr>
<td align="left">int</td>
<td align="left">MPI_INT</td>
</tr>
<tr>
<td align="left">long</td>
<td align="left">MPI_LONG</td>
</tr>
<tr>
<td align="left">float</td>
<td align="left">MPI_FLOAT</td>
</tr>
<tr>
<td align="left">double</td>
<td align="left">MPI_DOUBLE</td>
</tr>
</tbody></table>
<p>然而，随着我们将更多地接触MPI，我们将探索，还有一种方法可以让用户定义自己的派生数据类型。例如，如果我们在C语言中使用结构，那么我们可以将该结构定义为一个新的MPI数据类型。这被证明是非常有用的，因为我们可以直接在一个消息中发送所有的东西。因此，这就不需要我们用不同的消息来发送结构的部分内容。但我们将在未来几周内深入研究派生数据类型。对于今天的部分，我们只使用简单的数据.类型。</p>
<h3 id="五、Quiz"><a href="#五、Quiz" class="headerlink" title="五、Quiz"></a>五、Quiz</h3><p><strong>通信器有什么作用？</strong><br>它可以防止你的主程序的MPI调用与库的MPI调用相混淆✅<br>它可以用来识别将参与消息传递的进程的子组✅<br>如果等于MPI_COMM_WORLD，它表明通信涉及所有进程✅<br>以上都是✅</p>
<p><strong>以下哪项是消息传递调用所不需要的。</strong><br>你的消息的起始内存地址<br>消息的类型<br>消息的大小，以字节数计✅<br>消息中的数据元素的数量</p>
<p><strong>在消息传递调用中，参数标签是什么意思。</strong><br>传入消息的消息类型<br>通信方式的类型<br>一个用户分配的号码，在发送方和接收方都必须匹配✅<br>进程组的类型</p>
<h2 id="第二节、Types-of-communication-in-MPI"><a href="#第二节、Types-of-communication-in-MPI" class="headerlink" title="第二节、Types of communication in MPI"></a>第二节、<strong>Types of communication in MPI</strong></h2><h3 id="一、Different-types-of-communication-in-MPI"><a href="#一、Different-types-of-communication-in-MPI" class="headerlink" title="一、Different types of communication in MPI"></a>一、Different types of communication in MPI</h3><p>我们可以通过两个标准来划分MPI中的通信类型。</p>
<p><strong>定义通信类型的第一个方法是根据所涉及的过程的数量来划分。</strong></p>
<ul>
<li>因此，如果只有两个进程参与通信，即只有一个发送方和一个接收方，那么它就被称为点对点通信。这是消息传递的最简单形式，一个进程向另一个进程发送消息。</li>
<li>这类通信的另一种类型是集体通信，我们有多个进程参与其中。这涉及到一个进程与其他许多进程之间的通信，甚至是许多进程与其他几个进程的通信。因此，这可以有不同的执行方式。这是区分通信类型的一个标准，即通过参与的进程的数量来区分。</li>
</ul>
<p><strong>第二个标准，也许是更复杂的标准，是通过将通信类型定义为阻塞型和非阻塞型。</strong></p>
<ul>
<li><p>一个阻塞式程序只有在操作完成后才会返回。这意味着阻塞意味着，如果我们发送一个消息，我们不能进行下一步的操作，直到接收方真正向我们返回它已经收到消息的信息。</p>
<img src="/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204164807103.png" srcset="/img/loading.gif" lazyload alt="image-20221204164807103" style="zoom:50%;">
</li>
<li><p>非阻塞式通信比简单的阻塞式通信更复杂。在这种情况下，它立即返回并允许子程序执行其他工作。它与阻塞式通信的不同之处在于，如果我们向接收方发送一些东西，我们可以在中间执行一些其他任务，在一段时间后，我们可以检查接收方是否真正返回了信息，即它已经收到了信息，或者一切正常。许多真实的应用通常采用这种类型的通信。</p>
</li>
</ul>
<img src="/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204164824037.png" srcset="/img/loading.gif" lazyload alt="image-20221204164824037" style="zoom:50%;">

<h4 id="Point-to-Point-Communication"><a href="#Point-to-Point-Communication" class="headerlink" title="Point-to-Point Communication"></a>Point-to-Point Communication</h4><p>正如我们在上一节已经看到的，点对点通信是最简单的通信，因为它只涉及两个进程。其中一个进程作为发送方，另一个作为接收方。在这里，源头或发送方通过通信器向接收方发送一个消息。为了做到这一点，环境必须知道谁是发送方，谁是接收方，这一点可以通过指定进程的等级来解决。</p>
<img src="/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204165627157.png" srcset="/img/loading.gif" lazyload alt="image-20221204165627157" style="zoom:50%;">

<h5 id="Sending-paradigms"><a href="#Sending-paradigms" class="headerlink" title="Sending paradigms"></a>Sending paradigms</h5><p>为了用点对点通信发送一个信息，我们使用函数</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-constructor">MPI_Send(<span class="hljs-params">void</span> <span class="hljs-operator">*</span><span class="hljs-params">buf</span>, <span class="hljs-params">int</span> <span class="hljs-params">count</span>, MPI_Datatype <span class="hljs-params">datatype</span>, <span class="hljs-params">int</span> <span class="hljs-params">dest</span>, <span class="hljs-params">int</span> <span class="hljs-params">tag</span>, MPI_Comm <span class="hljs-params">comm</span>)</span>;<br></code></pre></td></tr></table></figure>

<p>我们现在来看看这个程序到底需要什么参数。</p>
<ul>
<li>buf是数据的指针，有很多元素，每个元素都用数据类型描述。所以首先，我们必须指定我们想发送的数据的地址。这是一个指向发送方数据的指针，然后第二个参数实际上是我们发送的元素的数量。例如，如果我们只发送1个整数，大小将是1。如果你发送一个有100个整数的数组，这将是100，以此类推。这个函数希望得到的第三个参数是数据类型。所以，如果我们要发送一个整数，我们将不得不在这里指定MPI_INT，以此类推。</li>
<li>dest是目标进程的等级。在这个参数中，我们指定接收方的等级。因此，例如，在前面的例子中，这将是5。</li>
<li>tag是与消息一起发送的附加整数信息。tag基本上是一个数字，我们通过它来识别消息。所以，通常情况下，如果我们只发送一条消息，我们可以只把0标签放在那里，也可以把任何我们希望的数字放在那里。</li>
<li>最后一个参数是通信器，正如我们已经讨论过的，我们通常使用MPI_COMM_WORLD。</li>
<li>这些参数是MPI环境需要的最重要的信息，以便知道发送什么数据以及发送给谁。</li>
</ul>
<h5 id="Receiving-paradigms"><a href="#Receiving-paradigms" class="headerlink" title="Receiving paradigms"></a>Receiving paradigms</h5><p>正如我们所看到的，发送方需要一个函数来发送消息，显然，接收方必须调用接收函数。这意味着，为了使通信工作，我们需要两个进程和两个等级。因此，一个将调用MPI_Send函数，另一个将类似地调用MPI_Recv来接收。为了能够接收，我们使用函数</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-constructor">MPI_Recv(<span class="hljs-params">void</span> <span class="hljs-operator">*</span><span class="hljs-params">buf</span>, <span class="hljs-params">int</span> <span class="hljs-params">count</span>, MPI_Datatype <span class="hljs-params">datatype</span>, <span class="hljs-params">int</span> <span class="hljs-params">source</span>, <span class="hljs-params">int</span> <span class="hljs-params">tag</span>, MPI_Comm <span class="hljs-params">comm</span>, MPI_Status <span class="hljs-operator">*</span><span class="hljs-params">status</span>)</span>;<br></code></pre></td></tr></table></figure>

<p>这个函数需要的参数与MPI_Send函数类似。</p>
<ul>
<li>buf&#x2F;count&#x2F;datatype描述了接收的数据。在第一个参数中，我们指定数据的地址，即我们想接收数据的地址，同样，我们把数据类型放在这里。</li>
<li>source是发送方进程的等级。我们必须要有发送方进程的等级。在前面的例子中，我们会指定数字3，因为数字3的等级正试图向我们发送一个消息。</li>
<li>与MPI_Send类似，我们这里也有一个标签。确保我们将这个数字与发送方相匹配真的很重要。因此，如果发送方指定消息的标签为0，接收方也必须在这里指定同样的数字。否则，这将是一个无限循环，因为我们将不会收到任何东西，因为我们的接收函数调用与发送函数调用不匹配。</li>
<li>下一个参数是通信器，我们将再次使用MPI_COMM_WORLD。</li>
<li>最后一个参数目前对我们来说不是很重要，因为这是我们将在下一个练习中学习的东西。现在我们只使用</li>
</ul>
<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">MPI_STATUS_IGNORE</span><br></code></pre></td></tr></table></figure>

<img src="/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204203725379.png" srcset="/img/loading.gif" lazyload alt="image-20221204203725379" style="zoom:50%;">

<p>这里，左边是发送方，右边是接收方。我们假设发送方想把这个有n个浮点的缓冲阵列发送给其他进程。为此，它调用MPI_Send常规函数。正如我们已经知道的，第一个是数据的指针。所以，这就是发送缓冲区。然后，它需要指定数据的数量。下一个参数是MPI_FLOAT，我们需要确保这个数据与前面提到的数据一致。正如我们之前讨论的，这是环境定义的MPI数据类型，但它必须与这个数据相匹配，否则通信将无法进行。我们需要记住的另一件事是，这个数据类型必须与接收器相匹配。所以，当我们写这些函数时，我们必须小心，所有这些都必须匹配。现在，接收方必须调用具有相同数据类型的接收方函数。在这里，它必须首先定义一个数组，用来接收这些数据，即接收缓冲区。当然，通信器也必须是相同的，因为它们被绑定在同一个程序中。但我们通常使用MPI_COMM_WORLD通信器。下一个重要的部分是，标签必须匹配。最后，消息的类型或数据的类型必须匹配。</p>
<h3 id="二、Send-and-receive"><a href="#二、Send-and-receive" class="headerlink" title="二、Send and receive"></a>二、Send and receive</h3><p>在这个练习中，你将编写一个基本的MPI程序，使用MPI_Send和MPI_Recv例程将数字-1从进程0发送至进程1（ping）。</p>
<figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs abnf">if (rank <span class="hljs-operator">=</span><span class="hljs-operator">=</span> <span class="hljs-number">0</span>) &#123; ... &#125;<br>else if (rank <span class="hljs-operator">=</span><span class="hljs-operator">=</span> <span class="hljs-number">1</span>) &#123; ... &#125;<br></code></pre></td></tr></table></figure>



<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs c">%%file ping.c<br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;mpi.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span><br>&#123;<br>    <span class="hljs-type">int</span> i, rank;<br>    <span class="hljs-type">float</span> buffer[<span class="hljs-number">1</span>];<br>    MPI_Status status;<br><br>    MPI_Init(<span class="hljs-literal">NULL</span>, <span class="hljs-literal">NULL</span>);<br>    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);<br><br>    <span class="hljs-keyword">if</span> (rank == <span class="hljs-number">0</span>)<br>    &#123;<br>      <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;I am %i before send ping \n&quot;</span>, rank);<br>      MPI_Send(buffer, <span class="hljs-number">1</span>, MPI_FLOAT, <span class="hljs-number">1</span>, <span class="hljs-number">17</span>, MPI_COMM_WORLD);<br>    &#125;<br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (rank == <span class="hljs-number">1</span>)<br>    &#123;<br>      MPI_Recv(buffer, <span class="hljs-number">1</span>, MPI_FLOAT, <span class="hljs-number">0</span>, <span class="hljs-number">17</span>, MPI_COMM_WORLD, &amp;status);<br>      <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;I am %i after  recv ping \n&quot;</span>, rank);<br>    &#125;<br><br>    MPI_Finalize();<br>&#125;<br><br>output:<br>I am <span class="hljs-number">0</span> before send ping <br>I am <span class="hljs-number">1</span> after  recv ping <br></code></pre></td></tr></table></figure>



<h3 id="三、Ping-pong"><a href="#三、Ping-pong" class="headerlink" title="三、Ping pong"></a>三、Ping pong</h3><p>在这个练习中，你将得到使用MPI_Send和MPI_Recv程序的练习。<br>这是上一个练习的继续，当时我们从进程0向进程1发送一个平移。现在你将发送和接收多个消息。</p>
<p>练习</p>
<ol>
<li>两个进程来回乒乓一个令牌，不断递增直到达到一个给定的值。</li>
<li>进程0向进程1发送一个消息（ping）。</li>
<li>在收到这个消息后，进程1又向进程0发送了一个消息（pong）。</li>
<li>每发送一条消息，令牌就会递增1。</li>
<li>重复这个ping pong，直到token的值达到6，即3个ping和3个pong。</li>
</ol>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs c">%%file ping.c<br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;mpi.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span><br>&#123;<br>    <span class="hljs-type">int</span> i, rank;<br>    <span class="hljs-type">float</span> token[<span class="hljs-number">1</span>] = &#123;<span class="hljs-number">10</span>&#125;;<br>    MPI_Status status;<br><br>    MPI_Init(<span class="hljs-literal">NULL</span>, <span class="hljs-literal">NULL</span>);<br>    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);<br>      <span class="hljs-keyword">if</span> (rank == <span class="hljs-number">0</span>)<br>    &#123;<br>      <span class="hljs-keyword">while</span> (token[<span class="hljs-number">0</span>] &gt; <span class="hljs-number">6</span>) &#123;<br>          <br>          token[<span class="hljs-number">0</span>]--;<br>          MPI_Send(token, <span class="hljs-number">1</span>, MPI_FLOAT, <span class="hljs-number">1</span>, <span class="hljs-number">17</span>, MPI_COMM_WORLD);<br>          MPI_Recv(token, <span class="hljs-number">1</span>, MPI_FLOAT, <span class="hljs-number">1</span>, <span class="hljs-number">22</span>, MPI_COMM_WORLD, &amp;status);<br>          <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;I am %i after recv pong %f\n&quot;</span>, rank, token[<span class="hljs-number">0</span>]);<br>      &#125;<br>    &#125;<br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (rank == <span class="hljs-number">1</span>)<br>    &#123;<br>      <span class="hljs-keyword">while</span> (token[<span class="hljs-number">0</span>] &gt; <span class="hljs-number">6</span>) &#123;<br>          MPI_Recv(token, <span class="hljs-number">1</span>, MPI_FLOAT, <span class="hljs-number">0</span>, <span class="hljs-number">17</span>, MPI_COMM_WORLD, &amp;status);<br>          <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;I am %i after recv ping %f\n&quot;</span>, rank, token[<span class="hljs-number">0</span>]);<br><br>          token[<span class="hljs-number">0</span>]--;<br>          MPI_Send(token, <span class="hljs-number">1</span>, MPI_FLOAT, <span class="hljs-number">0</span>, <span class="hljs-number">22</span>, MPI_COMM_WORLD);<br>      &#125;<br>    &#125;<br><br><br>    MPI_Finalize();<br>&#125;<br><br>output:<br>I am <span class="hljs-number">0</span> after recv pong <span class="hljs-number">8.000000</span><br>I am <span class="hljs-number">0</span> after recv pong <span class="hljs-number">6.000000</span><br>I am <span class="hljs-number">1</span> after recv ping <span class="hljs-number">9.000000</span><br>I am <span class="hljs-number">1</span> after recv ping <span class="hljs-number">7.000000</span><br></code></pre></td></tr></table></figure>



<h3 id="四、What-happens-with-a-different-number-of-pings-and-pongs-🤔️"><a href="#四、What-happens-with-a-different-number-of-pings-and-pongs-🤔️" class="headerlink" title="四、What happens with a different number of pings and pongs ?🤔️"></a>四、What happens with a different number of pings and pongs ?🤔️</h3><p>上一步的程序对不同数量的ping和pongs，即3个ping和2个pongs，是否有效？</p>
<h3 id="五、Rotating-information-around-a-ring"><a href="#五、Rotating-information-around-a-ring" class="headerlink" title="五、Rotating information around a ring"></a>五、Rotating information around a ring</h3><p>在这个练习中，你将尝试使用阻塞式和非阻塞式通信。通过使用非阻塞通信，我们要避免闲置时间、死锁和序列化。这是两部分练习的第一部分。</p>
<p>这是一个环形通信的小例子，它是一种具有循环边界条件的晕轮通信。每个进程都在向其右边的邻居发送其数据，并从其左边的邻居那里接收数据（在一个环中）。如果我们使用阻塞式例程，就会有死锁和串行化的风险。所有进程都可能被卡在MPI_Send，因为它们在等待MPI_Recv的调用。</p>
<p>你从提供的源代码开始，这是错误的，因为它以一种天真的方式使用MPI_Send和MPI_Recv。如果MPI_Send是用同步通信协议实现的，那么这个程序就会出现死锁。但是我们期望这个错误的程序能够工作，因为我们只发送一个小的1整数信息而不是巨大的双精度数组。下面的程序在一个循环中以进程迭代的方式发送环形的等级值，并将所有的数值相加（所有等级之和）。邻居等级的计算是通过modulo运算完成的。我们使用2个不同的缓冲区进行发送和接收，这通常是在我们进行非阻塞通信时使用。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;mpi.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span><br>&#123;<br>    <span class="hljs-type">int</span> rank, size;<br>    <span class="hljs-type">int</span> snd_buf, rcv_buf;<br>    <span class="hljs-type">int</span> right, left;<br>    <span class="hljs-type">int</span> sum, i;<br>    MPI_Status status;<br><br>    MPI_Init(<span class="hljs-literal">NULL</span>, <span class="hljs-literal">NULL</span>);<br>    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);<br>    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);<br><br>    right = (rank+<span class="hljs-number">1</span>)      % size;<br>    left  = (rank<span class="hljs-number">-1</span>+size) % size;<br><br>    sum = <span class="hljs-number">0</span>;<br>    snd_buf = rank; <span class="hljs-comment">//store rank value in send buffer</span><br>    <span class="hljs-keyword">for</span>(i = <span class="hljs-number">0</span>; i &lt; size; i++) <br>    &#123;<br>        MPI_Send(&amp;snd_buf, <span class="hljs-number">1</span>, MPI_INT, right, <span class="hljs-number">17</span>, MPI_COMM_WORLD); <span class="hljs-comment">//send data to the right neighbour</span><br>        MPI_Recv(&amp;rcv_buf, <span class="hljs-number">1</span>, MPI_INT, left,  <span class="hljs-number">17</span>, MPI_COMM_WORLD, &amp;status); <span class="hljs-comment">//receive data from the left neighbour</span><br>        snd_buf = rcv_buf; <span class="hljs-comment">//prepare send buffer for next iteration</span><br>        sum += rcv_buf; <span class="hljs-comment">//sum of all received values</span><br>    &#125;<br>    <span class="hljs-built_in">printf</span> (<span class="hljs-string">&quot;PE%i:\tSum = %i\n&quot;</span>, rank, sum);<br><br>    MPI_Finalize();<br>&#125;<br></code></pre></td></tr></table></figure>

<p>练习</p>
<ul>
<li>进入练习，用进程3、4、5运行并检查正确的和。</li>
<li>然后用MPI_Send (explicit synchronous send) 代替MPI_Send。运行该程序。你将看到一个死锁，你将需要杀死程序（用中断按钮◼）。</li>
<li>用序列化来解决死锁问题。使用技巧：例如，如果等级&#x3D;&#x3D; ，则先做接收，然后再发送。用3、4、5个进程运行程序，看看你是否解决了死锁问题。现在用1个进程运行该程序。它仍然会出现死锁。由于性能不好，该程序仍然是错误的，当只用1个进程运行时，它仍然会死锁。</li>
<li>为什么我们用一个有2个不同缓冲区的程序而不是1个缓冲区的序列化方案？</li>
<li>为什么序列化的方案在运行1个进程时仍然会死锁？</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs c">%%file ring.c<br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;mpi.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span><br>&#123;<br>    <span class="hljs-type">int</span> rank, size;<br>    <span class="hljs-type">int</span> snd_buf, rcv_buf;<br>    <span class="hljs-type">int</span> right, left;<br>    <span class="hljs-type">int</span> sum, i;<br>    MPI_Status status;<br><br>    MPI_Init(<span class="hljs-literal">NULL</span>, <span class="hljs-literal">NULL</span>);<br>    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);<br>    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);<br><br>    right = (rank+<span class="hljs-number">1</span>)      % size;<br>    left  = (rank<span class="hljs-number">-1</span>+size) % size;<br><br>    sum = <span class="hljs-number">0</span>;<br>    snd_buf = rank;<br>    <span class="hljs-keyword">for</span>(i = <span class="hljs-number">0</span>; i &lt; size; i++) <br>    &#123;<br>        <span class="hljs-keyword">if</span> (rank == <span class="hljs-number">0</span>) &#123;<br>            MPI_Ssend(&amp;snd_buf, <span class="hljs-number">1</span>, MPI_INT, right, <span class="hljs-number">17</span>, MPI_COMM_WORLD);<br>            MPI_Recv(&amp;rcv_buf, <span class="hljs-number">1</span>, MPI_INT, left,  <span class="hljs-number">17</span>, MPI_COMM_WORLD, &amp;status);<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            MPI_Recv(&amp;rcv_buf, <span class="hljs-number">1</span>, MPI_INT, left,  <span class="hljs-number">17</span>, MPI_COMM_WORLD, &amp;status);<br>            MPI_Ssend(&amp;snd_buf, <span class="hljs-number">1</span>, MPI_INT, right, <span class="hljs-number">17</span>, MPI_COMM_WORLD);<br>        &#125;<br>        snd_buf = rcv_buf;<br>        sum += rcv_buf;<br>    &#125;<br>    <span class="hljs-built_in">printf</span> (<span class="hljs-string">&quot;PE%i:\tSum = %i\n&quot;</span>, rank, sum);<br><br>    MPI_Finalize();<br>&#125;<br><br>output：<br>PE0:	Sum = <span class="hljs-number">3</span><br>PE1:	Sum = <span class="hljs-number">3</span><br>PE2:	Sum = <span class="hljs-number">3</span><br></code></pre></td></tr></table></figure>



<h3 id="六、Dynamic-Receiving-with-MPI-PROBE-and-MPI-STATUS"><a href="#六、Dynamic-Receiving-with-MPI-PROBE-and-MPI-STATUS" class="headerlink" title="六、Dynamic Receiving with MPI PROBE and MPI STATUS"></a>六、Dynamic Receiving with MPI PROBE and MPI STATUS</h3><p>在前面的练习中，我们实现了沿环形线发送数组的程序。这是一个应用的例子，我们已经知道我们将使用一个有100个值的数组。我们已经知道它有多长，或者换句话说，这个信息有多大，有多少元素被实际发送等等。这很重要，因为如果我们回过头来看发送和接收例程，我们需要指定这个计数。所以，不知道这些数字已经占了一个问题。在本小节中，我们将了解到有两种方法来处理这种情况，也就是说，如果不知道消息的大小。</p>
<ul>
<li>第一种方法是将数据的大小作为一个单独的发送&#x2F;回传操作来发送。因此，我们会用MPI_Send发送一个单独的消息，在这里我们可以发送例如一个数组中的元素数量，而我们稍后会将其发送出去。这样做是可行的，但有时效率并不高。</li>
<li>第二种也是更有效的方法是我们将在本小节中借助两个函数来学习的。我们将通过使用MPI_Probe和MPI_Status来获得发送数据的大小来解决这个问题。</li>
</ul>
<p>为了更多地了解这些函数以及如何使用它们，我们需要掌握以下一些概念。</p>
<h4 id="Wildcards"><a href="#Wildcards" class="headerlink" title="Wildcards"></a>Wildcards</h4><p>到目前为止，当我们在调用MPI_Send和MPI_Recv函数时，我们一直在指定一些信息。在这些情况下，接收器可以在其中一些参数中使用通配符。这意味着在函数中</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-constructor">MPI_Recv(<span class="hljs-params">void</span> <span class="hljs-operator">*</span><span class="hljs-params">buf</span>, <span class="hljs-params">int</span> <span class="hljs-params">count</span>, MPI_Datatype <span class="hljs-params">datatype</span>, <span class="hljs-params">int</span> <span class="hljs-params">source</span>,<span class="hljs-params">int</span> <span class="hljs-params">tag</span>, MPI_Comm <span class="hljs-params">comm</span>, MPI_Status <span class="hljs-operator">*</span><span class="hljs-params">status</span>)</span>;<br></code></pre></td></tr></table></figure>

<p>接收者可以直接把源头作为</p>
<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">MPI_ANY_SOURCE</span><br></code></pre></td></tr></table></figure>

<p>这意味着它不关心消息从哪里来，允许从任何来源接收。例如，如果你们都给我发了一条信息，而我只想阅读其中一条信息，我就会使用这个。所以，我不会关心谁给我发的信息，我只是从其中一个来源阅读它。同样地，我们也可以对标签这样做。在我们必须提及标签号码的函数中，我们可以直接使用</p>
<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">MPI_ANY_TAG</span><br></code></pre></td></tr></table></figure>

<p>允许我们接收具有任何标签的信息。</p>
<h4 id="MPI-Status-and-MPI-Probe"><a href="#MPI-Status-and-MPI-Probe" class="headerlink" title="MPI_Status and MPI_Probe"></a><code>MPI_Status</code> and <code>MPI_Probe</code></h4><p>MPI_Status是一个包含重要信息的结构，如以下信息。</p>
<ul>
<li>The rank of the sender as</li>
</ul>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs fortran"><span class="hljs-keyword">status</span>.MPI_SOURCE<br></code></pre></td></tr></table></figure>

<ul>
<li>The tag of the message as</li>
</ul>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs fortran"><span class="hljs-keyword">status</span>.MPI_TAG<br></code></pre></td></tr></table></figure>

<ul>
<li>The length of the message with</li>
</ul>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-constructor">MPI_Get_count(MPI_Status <span class="hljs-operator">*</span><span class="hljs-params">status</span>, MPI_Datatype <span class="hljs-params">datatype</span>, <span class="hljs-params">int</span> <span class="hljs-operator">*</span><span class="hljs-params">count</span>)</span><br></code></pre></td></tr></table></figure>

<p>所以，这个Status结构有一些属性，可以为我们提供关于发送方的额外信息。例如，就像我们看到的那样，如果我们使用MPI_ANY_SOURCE或MPI_ANY_TAG，那么在这种情况下，Status是我们能够找出发件人究竟是谁的唯一方法。所以，使用前两者我们可以得到谁是发件人和消息的标签的信息。在后面的练习中我们还将看到，我们可以使用Status结构来调用MPI_Get_count函数，该函数实际上给我们提供了消息的大小。因此，如果我们不知道有多少个数字，或者不知道消息会有多大，我们可以使用MPI_Get_count函数来推断出消息的尺寸是多少。我们很快就会在一个例子中看到这个方法的作用。</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-constructor">MPI_Probe(<span class="hljs-params">int</span> <span class="hljs-params">source</span>, <span class="hljs-params">int</span> <span class="hljs-params">tag</span>, MPI_Comm <span class="hljs-params">comm</span>, MPI_Status<span class="hljs-operator">*</span> <span class="hljs-params">status</span>)</span>;<br></code></pre></td></tr></table></figure>

<p>它可以被认为是一个MPI_Recv，除了接收消息，它做了所有的事情，但我们可以从中得到很多信息。一旦我们有了这些信息，我们就可以实际使用MPI_Recv来接收实际的消息。<br>我们可以通过下面的可选练习更好地理解这些函数。</p>
<h2 id="第三节、Collective-communications"><a href="#第三节、Collective-communications" class="headerlink" title="第三节、Collective communications"></a>第三节、<strong>Collective communications</strong></h2><h3 id="一、Basic-collective-communications"><a href="#一、Basic-collective-communications" class="headerlink" title="一、Basic collective communications"></a>一、Basic collective communications</h3><p>到目前为止，我们学习的所有不同的方法和练习实际上只涉及点对点的通信，即两个过程之间的通信。现在我们将介绍一些更高级的通信，其中涉及到更多的进程。我们必须记住，在这些涉及一组进程的通信中，例程被一个通信器中的所有进程调用。所以，简单地说，参与这种通信的所有进程都需要调用同一个例程。</p>
<p>Examples of these types of communication are</p>
<ul>
<li>Broadcast</li>
<li>Scatter</li>
<li>Gather</li>
<li>Reduction</li>
</ul>
<h4 id="Broadcast"><a href="#Broadcast" class="headerlink" title="Broadcast"></a>Broadcast</h4><p>广播是所有集体通信形式中最简单的一种，它被用来向所有进程发送用户输入或参数。广播的基本思想是，我们选择一个进程向通信器中的所有其他进程发送相同的数据。通常我们在算法的开始阶段使用广播，在这种情况下，我们想分配用户输入或算法的一些参数。例如，如果我们不使用并行IO（输入-输出），通常的情况是，我们用一个处理器读取一个文件，然后用广播分配内容。<br>为了执行这个，我们将使用函数</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-constructor">MPI_Bcast(<span class="hljs-params">void</span><span class="hljs-operator">*</span> <span class="hljs-params">data</span>, <span class="hljs-params">int</span> <span class="hljs-params">count</span>, MPI_Datatype <span class="hljs-params">datatype</span>, <span class="hljs-params">int</span> <span class="hljs-params">root</span>, MPI_Comm <span class="hljs-params">communicator</span>)</span>;<br></code></pre></td></tr></table></figure>

<p>与我们到目前为止所学的大多数函数类似，在广播函数的参数中，我们想指定数据，所以我们想分享的数据的指针。同样，我们指定有多少个数据，以及它的类型是什么。然而，我们将注意到的一个有趣的区别是，这里没有发送方源和接收方。但实际上这个根告诉我们的是，在这里我们指定分配数据的进程的等级。所有不是根的其他进程都会收到消息。虽然根进程和接收进程做的是不同的工作，但它们都调用了同一个MPI_Bcast函数! 我们将通过一个例子更好地理解它。在参数的最后，我们必须指定通信器。</p>
<p>例如，假设我们已经用五个处理器初始化了MPI应用程序，这意味着等级0到4。现在我们想让等级1发送一个字符串，一个包含字母r-e-d的字符数组给所有其他进程。如前所述，为了使其发挥作用，等级1以及我们希望参与这一通信的所有其他进程都必须调用同一个函数和这个函数的原型。</p>
<img src="/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221204215929294.png" srcset="/img/loading.gif" lazyload alt="image-20221204215929294" style="zoom:50%;">

<p>所以，这里假设红色被包含在名为buf的数组中。有三个元素我们想在所有其他进程中分配。它们的数据类型是字符，所以我们使用MPI_CHAR。root是1，所以发送或广播数据的进程有等级1，其次是通信器MPI_COMM_WORLD。因此，上面的图片的函数将是</p>
<figure class="highlight objectivec"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs objectivec"><span class="hljs-built_in">MPI_Bcast</span>(buf, <span class="hljs-number">3</span>, <span class="hljs-built_in">MPI_CHAR</span>, <span class="hljs-number">1</span>, <span class="hljs-built_in">MPI_COMM_WORLD</span>);<br></code></pre></td></tr></table></figure>

<p>我们可以看到，这是一个相当简单但非常有用的程序。请记住，这个确切的函数调用必须由每个参与的进程调用，根和所有的接收进程。</p>
<h3 id="二、Broadcast"><a href="#二、Broadcast" class="headerlink" title="二、Broadcast"></a>二、Broadcast</h3><p>在这个练习中，你将得到使用MPI_Bcast和编写你自己的广播函数。你将比较MPI和你的函数的时间效率。</p>
<p>练习</p>
<ol>
<li><p>进入练习，完成使用MPI_Bcast例程从等级为0的进程中广播一个有10.000.000个数字的阵列的程序。</p>
</li>
<li><p>使用MPI_Send和MPI_Recv例程编写你自己的广播函数。</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">void my<span class="hljs-constructor">_Bcast(<span class="hljs-params">void</span><span class="hljs-operator">*</span> <span class="hljs-params">data</span>, <span class="hljs-params">int</span> <span class="hljs-params">count</span>, MPI_Datatype <span class="hljs-params">datatype</span>, <span class="hljs-params">int</span> <span class="hljs-params">root</span>, MPI_Comm <span class="hljs-params">communicator</span>)</span>;<br></code></pre></td></tr></table></figure>
</li>
<li><p>使用MPI函数MPI_Wtime测量两个例程在2、4、8个处理器上运行时的时间。你对不同规模下的差异有什么看法？</p>
</li>
</ol>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arduino"><span class="hljs-function"><span class="hljs-type">double</span> <span class="hljs-title">MPI_Wtime</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span></span>;<br></code></pre></td></tr></table></figure>



<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><code class="hljs c">%%file bcast.c<br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;mpi.h&gt;</span></span><br><br><span class="hljs-type">void</span> <span class="hljs-title function_">my_Bcast</span><span class="hljs-params">(<span class="hljs-type">void</span>* data, <span class="hljs-type">int</span> count, MPI_Datatype datatype, <span class="hljs-type">int</span> root, MPI_Comm communicator)</span> <br>&#123;<br>    <span class="hljs-type">int</span> rank, size, i;<br>    MPI_Comm_rank(communicator, &amp;rank);<br>    MPI_Comm_size(communicator, &amp;size);<br>    <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span></span><br>    <span class="hljs-comment">// If we are the root process, send our data to everyone</span><br>    <span class="hljs-comment">// If we are a receiver process, receive the data from the root</span><br>    <span class="hljs-keyword">if</span>(rank == root)&#123;<br>        <span class="hljs-comment">// send</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; size; i++)&#123;<br>            <span class="hljs-keyword">if</span>(i != root)&#123;<br>                MPI_Send(data, count,datatype, i ,<span class="hljs-number">0</span>, communicator);<br>            &#125;<br>        &#125;<br>        <br>    &#125;<span class="hljs-keyword">else</span>&#123;<br>        MPI_Recv(data, count, datatype, root,<span class="hljs-number">0</span> ,communicator, MPI_STATUS_IGNORE );<br>    &#125;<br>    <br>&#125;<br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span><br>&#123;<br>    <span class="hljs-type">int</span> rank, i;<br>    <span class="hljs-type">int</span> num_elements = <span class="hljs-number">10000000</span>; <span class="hljs-comment">// size of array</span><br>    <span class="hljs-type">int</span> num_trials = <span class="hljs-number">10</span>; <span class="hljs-comment">// number of timing experiments</span><br><br>    MPI_Init(<span class="hljs-literal">NULL</span>, <span class="hljs-literal">NULL</span>);<br>    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);<br><br>    <span class="hljs-type">double</span> total_my_bcast_time = <span class="hljs-number">0.0</span>;<br>    <span class="hljs-type">double</span> total_mpi_bcast_time = <span class="hljs-number">0.0</span>;<br>    <span class="hljs-type">int</span>* data = (<span class="hljs-type">int</span>*)<span class="hljs-built_in">malloc</span>(<span class="hljs-keyword">sizeof</span>(<span class="hljs-type">int</span>) * num_elements); <span class="hljs-comment">// create array</span><br><br>    <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; num_trials; i++) &#123;<br>        <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span></span><br>        <span class="hljs-comment">// broadcast with MPI_Bcast</span><br>        <span class="hljs-comment">// time MPI_Bcast</span><br>        <span class="hljs-comment">// synchronize before starting timing and before obtaining final time</span><br>        MPI_Barrier(MPI_COMM_WORLD);<br>        total_mpi_bcast_time -= MPI_Wtime();<br>        MPI_Bcast(data, num_elements, MPI_INT, <span class="hljs-number">0</span>, MPI_COMM_WORLD);<br><br>        MPI_Barrier(MPI_COMM_WORLD);<br>        total_mpi_bcast_time += MPI_Wtime();<br><br>        <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span></span><br>        <span class="hljs-comment">// broadcast with my_Bcast</span><br>        <span class="hljs-comment">// time my_Bcast</span><br>        MPI_Barrier(MPI_COMM_WORLD);<br>        total_my_bcast_time -= MPI_Wtime();<br>        my_Bcast(data, num_elements, MPI_INT, <span class="hljs-number">0</span>, MPI_COMM_WORLD);<br>        MPI_Barrier(MPI_COMM_WORLD);<br>        total_my_bcast_time += MPI_Wtime();<br><br>    &#125;<br><br>    <span class="hljs-comment">// Print resulting times</span><br>    <span class="hljs-keyword">if</span> (rank == <span class="hljs-number">0</span>) &#123;<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Avg my_Bcast time = %lf\n&quot;</span>, total_my_bcast_time / num_trials);<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Avg MPI_Bcast time = %lf\n&quot;</span>, total_mpi_bcast_time / num_trials);<br>    &#125;<br><br>    <span class="hljs-built_in">free</span>(data);<br>    MPI_Finalize();<br>&#125;<br><br>output:(n = <span class="hljs-number">4</span>)<br>Avg my_Bcast time = <span class="hljs-number">0.017896</span><br>Avg MPI_Bcast time = <span class="hljs-number">0.022628</span><br></code></pre></td></tr></table></figure>

<p>自定义的反而要快？❌？</p>
<p>只要我们有&gt;2个进程，MPI_Bcast的执行时间就比my_Bcast的执行时间小，而且它的扩展性更好。<br>n &#x3D; [2, 4, 6, 8, 12, 16] # 进程<br>t1 &#x3D; [1.9e-2, 5.6e-2, 0.11, 0.13, 0.18, 0.22] # MPI_Bcast<br>t2 &#x3D; [1.1e-2, 8.9e-2, 0.19, 0.40, 0.84, 1.51] # my_Bcast</p>
<p>![img](Message Passing Interface&#x2F;hero_db3fb5e7-3d36-475e-af4d-44fa31d69607.png)</p>
<h3 id="四、Scatter-and-Gather"><a href="#四、Scatter-and-Gather" class="headerlink" title="四、Scatter and Gather"></a>四、Scatter and Gather</h3><h4 id="散射"><a href="#散射" class="headerlink" title="散射"></a>散射</h4><p>函数的原型：</p>
<figure class="highlight objectivec"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs objectivec"><span class="hljs-built_in">MPI_Scatter</span> (<span class="hljs-type">void</span> *sendbuf, <span class="hljs-type">int</span> sendcount, <span class="hljs-built_in">MPI_Datatype</span> sendtype, <span class="hljs-type">void</span> *recvbuf, <span class="hljs-type">int</span> recvcount, <span class="hljs-built_in">MPI_Dataype</span> recvtype, <span class="hljs-type">int</span> root, <span class="hljs-built_in">MPI_Comm</span> comm)<br></code></pre></td></tr></table></figure>

<p>函数原型与广播类似，但我们将通过参数，因为有些部分我们需要小心处理。像往常一样，首先我们要指定数据，所以这就是缓冲区。在这个例子中，根处理器将是等级为1的那个，它想把这个由5个数字组成的数组分散到所有其他进程中。为了能够做到这一点，它将需要指定这个sendbuf。在这之后是数字<strong>sendcount</strong>，稍后我们会看到recvcount。通常情况下，它们是一样的。这实际上是一个数字，<strong>它告诉你有多少元素将被发送到每个进程</strong>，重要的是要注意，它并不意味着总共有多少元素被发送，而只是每个进程将得到的那一部分。下一个参数是recvbuf，它是将接收数据的进程的缓冲区。最后，root与广播中的参数相同。它是实际进行散布的进程，而comm表示进程所在的通信器。在这个函数中，我们唯一需要注意的是sendcount和recvcount，因为这是决定将向每个进程发送多少个元素的数字，而不是整个元素的数量。另一个需要注意的是，当这个函数完成后，发送方（在我们的例子中，等级为1的进程）将不会得到整个数据的信息。在我们的例子中，这将意味着在通信后，等级1将只拥有一部分数据，即B。</p>
<img src="/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221208170337715.png" srcset="/img/loading.gif" lazyload alt="image-20221208170337715" style="zoom:50%;">

<p>MPI_Bcast和MPI_Scatter之间的区别是，MPI_Bcast向所有进程发送相同的数据，而MPI_Scatter则向不同的进程发送大块的数据。</p>
<h4 id="收集"><a href="#收集" class="headerlink" title="收集"></a>收集</h4><p>正如我们将看到的，许多MPI应用的基本思想是，我们有一些数据，我们分散它，使每个进程计算一些东西，然后我们在一个进程中收集回信息。该函数与MPI_Scatter非常相似</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-constructor">MPI_Gather(<span class="hljs-params">void</span> <span class="hljs-operator">*</span><span class="hljs-params">sendbuf</span>, <span class="hljs-params">int</span> <span class="hljs-params">sendcount</span>, MPI_Datatype <span class="hljs-params">sendtype</span>, <span class="hljs-params">void</span> <span class="hljs-operator">*</span><span class="hljs-params">recvbuf</span>, <span class="hljs-params">int</span> <span class="hljs-params">recvcount</span>, MPI_Dataype <span class="hljs-params">recvtype</span>, <span class="hljs-params">int</span> <span class="hljs-params">root</span>, MPI_Comm <span class="hljs-params">comm</span>)</span><br></code></pre></td></tr></table></figure>

<p>这里的主要区别是，由于只有一个进程，即根进程，收集所有的信息，它是唯一需要有一个有效的接收缓冲区的进程。所有其他的调用进程可以为recvbuf传递NULL，因为他们不接收任何东西，因为他们只是将数据发送到根进程。最后，再次要注意和记住的是，<strong>recvcount参数是每个进程收到的元素的数量，而不是所有进程的总和</strong></p>
<img src="/MPI/Introduction-to-Parallel-Programming/Message%20Passing%20Interface/image-20221208170844247.png" srcset="/img/loading.gif" lazyload alt="image-20221208170844247" style="zoom:50%;">

<h3 id="五、Scatter-and-Gather"><a href="#五、Scatter-and-Gather" class="headerlink" title="五、Scatter and Gather"></a>五、Scatter and Gather</h3><p>在这个练习中，你将编写一个MPI程序，使用MPI_Scatter和MPI_Gather计算一个数组元素的平均值。<br>这个相当简单的程序演示了如何使用MPI在不同的进程中进行分工，对数据的子集进行计算，然后将较小的结果合并到最终结果中。<br>该程序采取了以下步骤。</p>
<ol>
<li>根进程（进程0）创建一个生成随机数的数组。</li>
<li>将根进程的随机数分散到所有其他进程，给每个进程同等数量的数字。</li>
<li>每个进程计算其数字子集的平均值。</li>
<li>根进程收集每个单独的平均数，并在现在更小的数字数组上计算出总的平均数。</li>
</ol>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs c">%%file gather.c<br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdlib.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;time.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;sys/time.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;mpi.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span><br>&#123;<br>    <span class="hljs-type">int</span> i, rank, size;<br>    <span class="hljs-type">int</span> num_elements_per_proc = <span class="hljs-number">3</span>;<br>    <span class="hljs-type">int</span> num_elements;<br>    <span class="hljs-comment">// seed the random number generator</span><br>    srand(time(<span class="hljs-literal">NULL</span>));<br><br>    MPI_Init(<span class="hljs-literal">NULL</span>, <span class="hljs-literal">NULL</span>);<br><br>    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);<br>    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);<br><br>    <span class="hljs-comment">// create a random array of elements on the root process</span><br>    <span class="hljs-comment">// total size will be the number of elements per process times the number of processes</span><br>    <span class="hljs-type">float</span> *rand_nums = <span class="hljs-literal">NULL</span>;<br>    <span class="hljs-keyword">if</span> (rank == <span class="hljs-number">0</span>) &#123;<br>        num_elements = num_elements_per_proc * size;<br>        rand_nums = (<span class="hljs-type">float</span> *)<span class="hljs-built_in">malloc</span>(<span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>) * num_elements);<br>        <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; num_elements; i++) &#123;<br>            rand_nums[i] = (rand() / (<span class="hljs-type">float</span>)RAND_MAX);<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// for each process, create a buffer that will hold a subset of the array</span><br>    <span class="hljs-type">float</span> *sub_rand_nums = (<span class="hljs-type">float</span> *)<span class="hljs-built_in">malloc</span>(<span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>) * num_elements_per_proc);<br><br>    <span class="hljs-comment">// scatter the random numbers from the root process to all other processes</span><br>    MPI_Scatter(rand_nums, num_elements_per_proc, MPI_FLOAT, sub_rand_nums,<br>              num_elements_per_proc, MPI_FLOAT, <span class="hljs-number">0</span>, MPI_COMM_WORLD);<br><br>    <span class="hljs-comment">// compute the average of your subset</span><br>    <span class="hljs-type">float</span> sub_avg = <span class="hljs-number">0.f</span>;<br>    <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; num_elements_per_proc; i++) &#123;<br>        sub_avg += sub_rand_nums[i];<br>    &#125;<br>    sub_avg = sub_avg / num_elements_per_proc;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;I am process %i out of %i, average result = %f \n&quot;</span>, rank, size, sub_avg);<br><br>    <span class="hljs-comment">// gather all partial averages down to the root process</span><br>    <span class="hljs-type">float</span> *sub_avgs = <span class="hljs-literal">NULL</span>;<br>    <span class="hljs-keyword">if</span> (rank == <span class="hljs-number">0</span>) &#123;<br>        <span class="hljs-comment">// memory allocation needed only on root process</span><br>        sub_avgs = (<span class="hljs-type">float</span> *)<span class="hljs-built_in">malloc</span>(<span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>) * size);<br>    &#125;<br>    MPI_Gather(&amp;sub_avg, <span class="hljs-number">1</span>, MPI_FLOAT, sub_avgs, <span class="hljs-number">1</span>, MPI_FLOAT, <span class="hljs-number">0</span>, MPI_COMM_WORLD);<br><br>    <span class="hljs-comment">// compute the total average of all numbers from the partial averages</span><br>    <span class="hljs-keyword">if</span> (rank == <span class="hljs-number">0</span>) &#123;<br>        <span class="hljs-type">float</span> avg = <span class="hljs-number">0.f</span>;<br>        <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; size; i++) &#123;<br>            avg += sub_avgs[i];<br>        &#125;<br>        avg = avg / size;<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Avg of all elements is %f\n&quot;</span>, avg);<br><br>        <span class="hljs-comment">// compute average across the original data for comparison</span><br>        <span class="hljs-type">float</span> original_data_avg = <span class="hljs-number">0.f</span>;<br>        <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; num_elements; i++) &#123;<br>            original_data_avg += rand_nums[i];<br>        &#125;<br>        original_data_avg = original_data_avg / num_elements;<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Avg computed across original data is %f\n&quot;</span>, original_data_avg);<br><br>    &#125;<br><br>    <span class="hljs-comment">// Clean up</span><br>    <span class="hljs-keyword">if</span> (rank == <span class="hljs-number">0</span>) &#123;<br>        <span class="hljs-built_in">free</span>(rand_nums);<br>        <span class="hljs-built_in">free</span>(sub_avgs);<br>    &#125;<br>    <span class="hljs-built_in">free</span>(sub_rand_nums);<br><br>    MPI_Finalize();<br>&#125;<br></code></pre></td></tr></table></figure>



<h2 id="第四节、Advanced-Collective-operations"><a href="#第四节、Advanced-Collective-operations" class="headerlink" title="第四节、Advanced Collective operations"></a>第四节、<strong>Advanced Collective operations</strong></h2><h3 id="一、MPI-Reduce"><a href="#一、MPI-Reduce" class="headerlink" title="一、MPI_Reduce"></a>一、MPI_Reduce</h3><p>到目前为止，在基本的集体通信中，我们已经遇到了广播、散射和聚集。现在，我们可以转向更高级的集体通信，我们将讨论MPI_Reduce和MPI_Allreduce例程。<br>在我们进入这些例程之前，让我们在实践中修订一下<em>reduce</em>或data reduction的概念。简单地说，data reduction就是通过一些函数将一组数字缩减为一个更小的数字集。例如，假设我们有一个数字列表（1，2，3，4，5）。那么用sum函数还原这个数字列表就会产生。</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">sum</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>) = <span class="hljs-number">15</span><br></code></pre></td></tr></table></figure>

<p>同样地，如果我们使用另一个函数，比如说乘法，那么乘法减法将产生</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">multiply</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>) = <span class="hljs-number">120</span>.<br></code></pre></td></tr></table></figure>

<p>很简单，这就是MPI reduction函数的作用。</p>
<h4 id="MPI-Reduce"><a href="#MPI-Reduce" class="headerlink" title="MPI_Reduce"></a><code>MPI_Reduce</code></h4><p> MPI_Reduce基本上就是我们在上一个练习中所做的，并增加了一个功能。在某种程度上，reduce例程所做的事情基本上与分散&#x2F;聚集类似，但我们也指定了一个MPI函数，如sum、multiplication、maximum或类似的东西。我们将在后面看到，哪些函数是可用的，以及我们如何使用它们。因此，MPI库在这些数据上直接使用这些函数，立即给我们减少的结果。我们不需要调用收集程序，然后手动编程来获得总和，而是由库来为我们完成。因此，MPI_Reduce在每个进程上接收一个输入元素的数组，并向根进程返回一个输出元素的数组。输出元素包含减去的结果。</p>
<p>也许通过一个例子会更容易理解。</p>
<p>![MPI_Reduce](Message Passing Interface&#x2F;hero_ffcb98cd-1789-4c4d-ac99-8026fce723b5.png)</p>
<p>让我们假设我们要计算一个总和，但不同的数字分散在不同的进程中。如果我们会有我们的数字（1，2，3，4，5），我们会在这些数据上调用MPI_Reduce，我们还需要提到我们想Reduce数据的函数，比如说，和。然后根进程将得到总和作为结果。为了能够做到这一点，我们需要MPI_Reduce的原型</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-constructor">MPI_Reduce(<span class="hljs-params">void</span><span class="hljs-operator">*</span> <span class="hljs-params">send_data</span>, <span class="hljs-params">void</span><span class="hljs-operator">*</span> <span class="hljs-params">recv_data</span>, <span class="hljs-params">int</span> <span class="hljs-params">count</span>, MPI_Datatype <span class="hljs-params">datatype</span>, MPI_Op <span class="hljs-params">op</span>, <span class="hljs-params">int</span> <span class="hljs-params">root</span>, MPI_Comm <span class="hljs-params">communicator</span>)</span>;<br></code></pre></td></tr></table></figure>

<p>这个例程的参数与我们到目前为止看到的参数非常相似。所以，send_data参数是一个每个进程想要减少的元素的数组。接下来是recv_data，它只对等级为root的进程有意义，因为它包含了减少的结果。然后我们提到计数，即数据的数目或数量和数据类型。然而，这就是MPI_Reduce函数的不同之处。这里我们还在op参数中提到了操作，即我们希望应用于数据的操作。MPI库中的还原操作列表如下。</p>
<table>
<thead>
<tr>
<th align="left">Function</th>
<th align="left"><code>MPI_Op</code></th>
</tr>
</thead>
<tbody><tr>
<td align="left">Maximum</td>
<td align="left"><code>MPI_MAX</code></td>
</tr>
<tr>
<td align="left">Minimum</td>
<td align="left"><code>MPI_MIN</code></td>
</tr>
<tr>
<td align="left">Sum</td>
<td align="left"><code>MPI_SUM</code></td>
</tr>
<tr>
<td align="left">Product</td>
<td align="left"><code>MPI_PROD</code></td>
</tr>
<tr>
<td align="left">Logical AND</td>
<td align="left"><code>MPI_LAND</code></td>
</tr>
<tr>
<td align="left">Logical OR</td>
<td align="left"><code>MPI_LOR</code></td>
</tr>
</tbody></table>
<h3 id="二、Computing-average-with-MPI-Reduce"><a href="#二、Computing-average-with-MPI-Reduce" class="headerlink" title="二、Computing average with MPI_Reduce"></a>二、Computing average with MPI_Reduce</h3><p>在这个练习中，你将编写一个MPI程序，使用MPI_Reduce计算一个数组元素的平均值。<br>在上一课中，你用MPI_Scatter和MPI_Gather计算了平均数。使用MPI_Reduce使代码简化了很多。<br>该程序采取了以下步骤。</p>
<ol>
<li>每个进程创建一个生成随机数的数组。</li>
<li>每个进程执行local_sum计算。</li>
<li>使用MPI_SUM将local_sum减少到根进程（进程0）。</li>
<li>根进程计算最后的平均值。</li>
</ol>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs c">%%file reduce.c<br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdlib.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;time.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;sys/time.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;mpi.h&gt;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span><br>&#123;<br>    <span class="hljs-type">int</span> i, rank, size;<br>    <span class="hljs-type">int</span> num_elements_per_proc = <span class="hljs-number">3</span>;<br>    <span class="hljs-type">int</span> num_elements;<br>    <span class="hljs-comment">// seed the random number generator to get different results for each processor</span><br>    srand(time(<span class="hljs-literal">NULL</span>)*rank);<br><br>    MPI_Init(<span class="hljs-literal">NULL</span>, <span class="hljs-literal">NULL</span>);<br><br>    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);<br>    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);<br><br>    <span class="hljs-comment">// create a random array of elements on all processes</span><br>    <span class="hljs-type">float</span> *rand_nums = <span class="hljs-literal">NULL</span>;<br>    rand_nums = (<span class="hljs-type">float</span> *)<span class="hljs-built_in">malloc</span>(<span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>) * num_elements_per_proc);<br>    <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; num_elements_per_proc; i++) &#123;<br>        rand_nums[i] = (rand() / (<span class="hljs-type">float</span>)RAND_MAX);<br>    &#125;<br><br>    <span class="hljs-comment">// sum the numbers locally</span><br>    <span class="hljs-type">float</span> local_sum = <span class="hljs-number">0.f</span>;<br>    <span class="hljs-comment">// TODO</span><br>    <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; num_elements_per_proc; i++) &#123;<br>        local_sum += rand_nums[i];<br>    &#125;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Local sum for process %d : %f, avg = %f\n&quot;</span>,<br>             rank, local_sum, local_sum / num_elements_per_proc);<br><br><br>    <span class="hljs-comment">// reduce all of the local sums into the global sum on root process</span><br>    <span class="hljs-type">float</span> global_sum;<br>    <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span> reduce</span><br>        MPI_Reduce(&amp;local_sum, &amp;global_sum, <span class="hljs-number">1</span>, MPI_FLOAT, MPI_SUM, <span class="hljs-number">0</span>, MPI_COMM_WORLD);<br><br>    <span class="hljs-comment">// print the result</span><br>    <span class="hljs-keyword">if</span> (rank == <span class="hljs-number">0</span>) &#123;<br>        <span class="hljs-comment">// TODO</span><br>         <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Total sum = %f, avg = %f\n&quot;</span>, global_sum,<br>            global_sum / (size * num_elements_per_proc));<br>        <br>    &#125;<br><br>    <span class="hljs-comment">// clean up</span><br>    <span class="hljs-built_in">free</span>(rand_nums);<br><br>    MPI_Finalize();<br>&#125;<br></code></pre></td></tr></table></figure>



<h3 id="四、MPI-Allreduce"><a href="#四、MPI-Allreduce" class="headerlink" title="四、MPI_Allreduce"></a>四、MPI_Allreduce</h3><p>在我们之前学习的MPI_Reduce函数中，其思路是其中一个处理器将从不同的处理器中获取数据，使用一些MPI操作将它们组合起来，使用这个操作来减少数据并获得结果。MPI_Allreduce以一种奇特的方式从MPI_Reduce中脱颖而出：在Allreduce中，所有的进程都会得到这个结果。简单地说，我们在减少数据的同时，以某种方式广播结果。当然，我们自己也可以做到这一点，但是MPI_Allreduce库做到了这一点，不仅速度更快，而且对我们来说也更容易。<br>许多并行应用需要在所有进程中访问减少的结果，而不是只访问根进程。这就是MPI_Allreduce最常用的地方，因为它可以减少数值并将结果分配给所有进程。简单地说，Allreduce基本上是减少和广播函数的结合。MPI_Allreduce的原型与MPI_Reduce非常相似，它看起来像</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-constructor">MPI_Allreduce(<span class="hljs-params">void</span><span class="hljs-operator">*</span> <span class="hljs-params">send_data</span>, <span class="hljs-params">void</span><span class="hljs-operator">*</span> <span class="hljs-params">recv_data</span>, <span class="hljs-params">int</span> <span class="hljs-params">count</span>, MPI_Datatype <span class="hljs-params">datatype</span>, MPI_Op <span class="hljs-params">op</span>, MPI_Comm <span class="hljs-params">communicator</span>)</span>;<br></code></pre></td></tr></table></figure>

<p>然而，一个主要的区别是，这里的参数中没有根，因为所有的进程都会得到数据。其他的东西都是差不多的。我们有两个用于发送和接收数据的指针。我们有每个处理器发送的元素的数量，后面是数据类型。然后类似于reduce函数，我们有我们想用来减少数据的MPI操作，当然最后还有通信器。<br>由于我们已经熟悉了reduce函数，通过下面的练习，我们将更容易了解Allreduce。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/MPI/" class="category-chain-item">MPI</a>
  
  
    <span>></span>
    
  <a href="/categories/MPI/Introduction-to-Parallel-Programming/" class="category-chain-item">Introduction-to-Parallel-Programming</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/">#并行计算</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Message Passing Interface</div>
      <div>http://by.ecel.top/MPI/Introduction-to-Parallel-Programming/Message Passing Interface/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>刘笑笑</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年12月4日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/OS/cpu/" title="CPU、核心以及与线程之间对应">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CPU、核心以及与线程之间对应</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/Algorithm/Storage%20of%20negative%20numbers/" title="计算机中负数的存储">
                        <span class="hidden-mobile">计算机中负数的存储</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    

  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
